Stating a precise definition of \glqq{}Neural Network\grqq{} is quite involved. 
Hence, we will only speak about the most fundamental type of neural networks, the so-called feed forward neural networks. 
Even for these simple networks, a precise definition is rather lengthy. We have to introduce some notation first:
Let $l\in\N_{\geq2}$, $n_0,n_1\dots,n_l\in\N$ and for each $i\in\N, i\leq l$ 
choose an affine linear map $T^{(i)}:\R^{n_{i-1}}\to\R^{n_i}$ and a smooth map $a^{(i)}:\R\to\R$.
Denoting with $f^{(i)}:\R^{n_i}\to\R^{n_i}$ the function acting element wise by $a^{(i)}$ (i.e $f^{(i)}(x_1, \dots, x_{n_i})=(a^{(i)}(x_1),\dots, a^{(i)}(x_{n_i}))$), 
we define the composition 
\begin{equation*} 
\tilde{F}\coloneq T^{(l)}\circ f^{(l-1)}\circ T^{(l-1)}\circ\dots\circ f^{(2)}\circ T^{(2)}\circ f^{(1)}\circ T^{(1)}:\R^{n_0}\to\R^{n_l}.
\end{equation*}
Recall that for any affine linear map $L:\R^n\to\R^m$ there exist $b\in\R^m$ and $W\in \text{Mat}(n\times m,\R)$ such that 
$Lx=W^Tx+b$ for all $x\in\R^n$ (the transposition is just for our convenience later on). Hence, to each $T^{(i)}$ corresponds a matrix $W^{(i)}\in\text{Mat}(n_{i-1}\times n_i,\R)$ called the weight-matrix and a vector
$b^{(i)}\in\R^{n_i}$ called bias such that $T^{(i)}x=\left(W^{(i)}\right)^Tx + b^{(i)}$.
We introduce further notation: Obviously, we can think of $W^{(i)}$ as an element in $\R^{n_{i-1}n_i}$. 
Let $\theta=(W^{(1)},b^{(1)},\dots,W^{(l)},b^{(l)})\in\R^{n_0n_1}\times\R^{n_1}\times\dots\times\R^{n_{l-1}n_l}\times\R^{l}$. 
Clearly, $\tilde{F}$ depends on the choice of $T^{(l)}$ and hence on the choice of $\theta$. 
Correspondingly, for each such $\theta$ we can build a map $\tilde{F}=F_\theta$.
With this notation in mind, we are ready to define what a neural network should be: The map
\begin{equation*}
    F:\R^{n_0n_1}\times\R^{n_1}\times\dots\times\R^{n_{l-1}n_l}\times\R^{n_l}\to C(\R^{n_0},\R^{n_l}), \quad\theta\mapsto F_\theta
\end{equation*}
is called (feed forward) neural network with $l$ layers and activation function $a^{(i)}$ in layer $i$.

Though this definition might seem technical, it bears a visual explanation:
It is best explained with figure~\ref{fig:nn}. We can view $F_\theta:\R^{n_0}\to\R^{n_l}$ as a graph-like structure. Each coordinate $x_i$ 
of an input vector $x\in\R^{n_0}$ can be thought of as a node. Applying $f^{(1)}\circ T^{(1)}$ to $x$ yields a
new vector $y^1=f^{(1)}\circ T^{(1)}(x)\in\R^{n_1}$. The coordinates of $y^1$ can again be interpreted as nodes. All these nodes (coordinates) together constitute the new layer
of the network. Applying $f^{(2)}\circ T^{(2)}$ to $y^1$ yields a new layer $y^2$ and so on. The last layer $y^l$ is then given by $F_\theta(x)$.
Going from layer $i-1$ to the layer $i$ can be visualized as follows: 
According to the above definitions we have
\begin{align*}
    y^{i}_j&=a^{(i)}\left(T^{(i)}\left(y^{i-1}\right)\right)_j\\
    &=a^{(i)}\left(\sum_{k=1}^{n_{i-1}}\left(W^{(i)}\right)^T_{jk}y^{i-1}_k+b^{(i)}_j\right)\\
    &=a^{(i)}\left(\sum_{k=1}^{n_{i-1}}W^{(i)}_{kj}y^{i-1}_k+b^{(i)}_j\right).
\end{align*}
Up to application of $a^{(i)}$, the node $y^i_j$ is a weighted sum of all nodes $y^{i-1}_k$ in the previous layer (in addition to a constant bias value $b^{(i)}_j$).
The matrix element $W^{(i)}_{kj}$ describes how much the node $y^i_j$ is influenced by node $y^{i-1}_k$. 
We picture an edge from node $y^{i-1}_k$ to node $y^i_j$ which has the weight $W^{(i)}_{kj}$. Consequently, the matrix $W^{(i)}$ is called weight-matrix.
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
        node distance=0.4cm and 1.5cm,
        node/.style={circle, draw=gray!60, fill=gray!5, very thick, minimum size=7mm},
        node_missing/.style={circle, inner sep=0pt, fill=gray, minimum size=2mm},
        node_invisible/.style={circle, inner sep=0pt, fill=red, minimum size=0mm}
        ]

        %Nodes input layer
        \node[node] (1_1) {$x_1$};
        \node[node] (1_2) [below=of 1_1] {$x_2$};
        \node[node_missing] (m_1_1) [below=of 1_2] {};
        \node[node_missing] (m_1_2) [below=of m_1_1] {};
        \node[node_missing] (m_1_3) [below=of m_1_2] {};
        \node[node] (1_3) [below=of m_1_3] {$x_{n_0}$};

        %Nodes hidden layer 1
        \node[node] (2_1) [above right=of 1_1] {$y^1_1$};
        \node[node] (2_2) [below=of 2_1] {$y^1_2$};
        \node[node] (2_3) [below=of 2_2] {$y^1_3$};
        \node[node_missing] (m_2_1) [below=of 2_3] {};
        \node[node_missing] (m_2_2) [below=of m_2_1] {};
        \node[node_missing] (m_2_3) [below=of m_2_2] {};
        \node[node] (2_4) [below=of m_2_3] {$y^1_{n_1}$};

        %Nodes hidden layer 2
        \node[node] (3_1) [right=of 2_1] {$y^2_1$};
        \node[node] (3_2) [below=of 3_1] {$y^2_2$};
        \node[node] (3_3) [below=of 3_2] {$y^2_3$};
        \node[node_missing] (m_3_1) [below=of 3_3] {};
        \node[node_missing] (m_3_2) [below=of m_3_1] {};
        \node[node_missing] (m_3_3) [below=of m_3_2] {};
        \node[node] (3_4) [below=of m_3_3] {$y^2_{n_2}$};

        %missing hidden layers
        \node[node_missing] (m_4_1) [below right=1cm and 1.5cm of 3_1] {};
        \node[node_missing] (m_4_2) [below=1cm of m_4_1] {};
        \node[node_missing] (m_4_3) [below=1cm of m_4_2] {};
        \node[node_missing] (m_5_1) [right=0.3cm of m_4_1] {};
        \node[node_missing] (m_5_2) [below=1cm of m_5_1] {};
        \node[node_missing] (m_5_3) [below=1cm of m_5_2] {};
        \node[node_missing] (m_6_1) [right=0.3cm of m_5_1] {};
        \node[node_missing] (m_6_2) [below=1cm of m_6_1] {};
        \node[node_missing] (m_6_3) [below=1cm of m_6_2] {};
        
        %Nodes hidden layer 3
        \node[node] (7_1) [above right=1cm and 1.5cm of m_6_1] {$y^{l-1}_1$};
        \node[node] (7_2) [below=of 7_1] {$y^{l-1}_2$};
        \node[node] (7_3) [below=of 7_2] {};
        \node[node_missing] (m_7_1) [below=of 7_3] {};
        \node[node_missing] (m_7_2) [below=of m_7_1] {};
        \node[node_missing] (m_7_3) [below=of m_7_2] {};
        \node[node] (7_4) [below=of m_7_3] {$y^{l-1}_{n_{l-1}}$};

        %Nodes output layer
        \node[node] (8_1) [below right=of 7_1] {$y^{l}_1$};
        \node[node] (8_2) [below=of 8_1] {$y^{l}_2$};
        \node[node_missing] (m_8_1) [below=of 8_2] {};
        \node[node_missing] (m_8_2) [below=of m_8_1] {};
        \node[node_missing] (m_8_3) [below=of m_8_2] {};
        \node[node] (8_3) [below=of m_8_3] {$y^{l}_{n_l}$};

        %invisible nodes
        \node[node_invisible] (inv_1_1) [right=0.75cm of 3_1] {};
        \node[node_invisible] (inv_1_2) [below right=0.2cm and 0.75cm of 3_1] {};
        \node[node_invisible] (inv_1_3) [above right=0.2cm and 0.75cm of 3_4] {};
        \node[node_invisible] (inv_1_4) [right=0.75cm of 3_4] {};
        \node[node_invisible] (inv_1_5) [right=0.75cm of 3_2] {};
        \node[node_invisible] (inv_1_6) [right=0.75cm of 3_3] {};
        \node[node_invisible] (inv_2_1) [left=0.75cm of 7_1] {};
        \node[node_invisible] (inv_2_2) [below left=0.2cm and 0.75cm of 7_1] {};
        \node[node_invisible] (inv_2_3) [above left=0.2cm and 0.75cm of 7_4] {};
        \node[node_invisible] (inv_2_4) [left=0.75cm of 7_4] {};
        \node[node_invisible] (inv_2_5) [left=0.75cm of 7_2] {};
        \node[node_invisible] (inv_2_6) [left=0.75cm of 7_3] {};
        %connections with invisible nodes
        \draw[dashed, thick, gray]  (3_1) -- (inv_1_1);
        \draw[dashed, thick, gray]  (3_1) -- (inv_1_2);
        \draw[dashed, thick, gray]  (3_2) -- (inv_1_5);
        \draw[dashed, thick, gray]  (3_3) -- (inv_1_6);
        \draw[dashed, thick, gray]  (3_4) -- (inv_1_3);
        \draw[dashed, thick, gray]  (3_4) -- (inv_1_4);
        \draw[dashed, thick, gray]  (7_1) -- (inv_2_1);
        \draw[dashed, thick, gray]  (7_1) -- (inv_2_2);
        \draw[dashed, thick, gray]  (7_2) -- (inv_2_5);
        \draw[dashed, thick, gray]  (7_3) -- (inv_2_6);
        \draw[dashed, thick, gray]  (7_4) -- (inv_2_3);
        \draw[dashed, thick, gray]  (7_4) -- (inv_2_4);

        %connections input and first hidden layer
        \draw[-, thick, gray]  (1_1) -- (2_1);
        \draw[-, thick, gray]  (1_1) -- (2_2);
        \draw[-, thick, gray]  (1_1) -- (2_3);
        \draw[-, thick, gray]  (1_1) -- (2_4);
        \draw[-, thick, gray]  (1_2) -- (2_1);
        \draw[-, thick, gray]  (1_2) -- (2_2);
        \draw[-, thick, gray]  (1_2) -- (2_3);
        \draw[-, thick, gray]  (1_2) -- (2_4);
        \draw[-, thick, gray]  (1_3) -- (2_1);
        \draw[-, thick, gray]  (1_3) -- (2_2);
        \draw[-, thick, gray]  (1_3) -- (2_3);
        \draw[-, thick, gray]  (1_3) -- (2_4);

        %connections first and second hidden layer
        \draw[-, thick, gray]  (2_1) -- (3_1);
        \draw[-, thick, gray]  (2_1) -- (3_2);
        \draw[-, thick, gray]  (2_1) -- (3_3);
        \draw[-, thick, gray]  (2_1) -- (3_4);
        \draw[-, thick, gray]  (2_2) -- (3_1);
        \draw[-, thick, gray]  (2_2) -- (3_2);
        \draw[-, thick, gray]  (2_2) -- (3_3);
        \draw[-, thick, gray]  (2_2) -- (3_4);
        \draw[-, thick, gray]  (2_3) -- (3_1);
        \draw[-, thick, gray]  (2_3) -- (3_2);
        \draw[-, thick, gray]  (2_3) -- (3_3);
        \draw[-, thick, gray]  (2_3) -- (3_4);
        \draw[-, thick, gray]  (2_4) -- (3_1);
        \draw[-, thick, gray]  (2_4) -- (3_2);
        \draw[-, thick, gray]  (2_4) -- (3_3);
        \draw[-, thick, gray]  (2_4) -- (3_4);

        %connections output and last hidden layer
        \draw[-, thick, gray]  (7_1) -- (8_1);
        \draw[-, thick, gray]  (7_1) -- (8_2);
        \draw[-, thick, gray]  (7_1) -- (8_3);
        \draw[-, thick, gray]  (7_2) -- (8_1);
        \draw[-, thick, gray]  (7_2) -- (8_2);
        \draw[-, thick, gray]  (7_2) -- (8_3);
        \draw[-, thick, gray]  (7_3) -- (8_1);
        \draw[-, thick, gray]  (7_3) -- (8_2);
        \draw[-, thick, gray]  (7_3) -- (8_3);
        \draw[-, thick, gray]  (7_4) -- (8_1);
        \draw[-, thick, gray]  (7_4) -- (8_2);
        \draw[-, thick, gray]  (7_4) -- (8_3);
    \end{tikzpicture}
    \caption{Visualization of a feed forward neural network.}
    \label{fig:nn}
\end{figure}

With the visuals in mind, we can explain what neural networks are used for and how they are being used. 
The purpose of neural networks lies in the  approximation of unknown functions. 
Suppose we were given a function $G:\R^n\to\R^m$ from which we only know values at $D$-many ($D\in\N$) points $x_1,\dots,x_D\in\R^n$, 
i.e. only the values $y_1=G(x_1),\dots,y_D=G(x_D)$ are known.
Choose $l\in\N$, set $n_0=n$, $n_l=m$ and consider the neural network
\begin{equation*}
    F:\R^{n_0n_1}\times\R^{n_1}\times\dots\times\R^{n_{l-1}n_l}\times\R^{n_l}\to C(\R^{n_0},\R^{n_l}), \quad\theta\mapsto F_\theta.
\end{equation*}
We can then try to find an \glqq{}optimal\grqq{} $\theta$ such that $F_\theta\approx G$. 
To do so, we have to introduce a measure for how much $F_\theta$ differs from $G$. 
This is commonly called a cost/loss function, i.e. a function $C:\R^m\times\R^m\to\R$, that maps to a pair $(F_\theta(x_i), y_i)$ a real value
$C(F_\theta(x_i),y_i)$ that can be interpreted as a distance between the true value $y_i$ and the predicted value $F_\theta(x_i)$.
Finding an optimal $\theta$ is achieved by minimizing the map $\theta\to C(F_\theta(x_i),y_i)$ for all $i=1,\dots,D$.
This optimization is called \glqq{}training the neural network\grqq{}. 
There is a whole branch of mathematics about these kinds of optimization of problems. 
A common way to do this is by means of the gradient descent method.
Roughly speaking, the idea is the following: Calculate the gradient of the map $\theta\mapsto C(F_\theta(x_i),y_i)$ for a fixed $i=1,\dots,D$, 
and then change $\theta$ slightly in the opposite direction of the gradient.
How much $\theta$ is changed, depends on the so-called learning rate $\eta\in\R_+$. 
As $F_\theta$ is a large composition, computing the gradient basically comes down to repeated application of the chain rule. 
An efficient algorithm that does exactly this and is widely used is called backpropagation. We can repeat this step until we found a $\theta$ such that 
$C(F\theta(x_i),y_i)$ is sufficiently small for all $i=1,\dots,d$.

Furthermore, there are many more techniques and tools (like optimizers, batching, ...) to make the training more efficient, faster converging and less time-consuming. 
However, this section was intended to give only a rough overview. Hence, we will not go into more detail. 
For additional information, there is a vast amount of literature on these topics, see for example \cite{bookNN1}, \cite{bookNN2} and \cite{paperHyperparameter}.