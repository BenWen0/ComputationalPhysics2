{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphToData(dir, file_id):\n",
    "    locs = np.loadtxt(f\"{dir}/{file_id}.locs\")[:, :2] # 0<=locs<=1\n",
    "    n_nodes = 100\n",
    "    locs_int_2d = np.rint(locs*100).astype(int) # 0<= locs_int_2d <= 100\n",
    "    locs_int = locs_int_2d[:, 0]*101 + locs_int_2d[:, 1]\n",
    "    locs_int_max = 101*101\n",
    "    locs_enc = np.zeros(shape=(n_nodes, locs_int_max))\n",
    "    locs_enc[np.arange(n_nodes), locs_int] = 1\n",
    "    adj = list()\n",
    "\n",
    "    with open(f\"{dir}/{file_id}.adj\", \"r\") as file:\n",
    "        # Read each line in the file\n",
    "        n_line = 0\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            neighbors = []\n",
    "            if line != \"None\":\n",
    "                neighbors = [int(id) for id in line.split()]        \n",
    "            for neighbor in neighbors:\n",
    "                adj.append([n_line, neighbor])\n",
    "            n_line += 1\n",
    "    adj_list = np.array(adj, dtype=int)\n",
    "\n",
    "    adj_list_pt = torch.tensor(adj_list, dtype=torch.long) #shape=[num_edges, 2] <- needs to be reshaped (see pyg doc)\n",
    "\n",
    "    num_edges = len(adj_list)#\n",
    "    len_edges = np.sqrt(np.sum((locs[adj_list[:, 0]] - locs[adj_list[:, 1]])**2, axis=1))\n",
    "    label_ = [1, 0] if dir.split(\"/\")[-1] == \"no\" else [0, 1]\n",
    "    label = torch.tensor([label_], dtype=torch.float) #shape=[1, num_classes]\n",
    "    node_values = torch.tensor(locs, dtype=torch.float) #num_node_features = 2, shape=[num_nodes, num_node_features]\n",
    "    edge_values = torch.tensor(len_edges.reshape(-1, 1), dtype=torch.float)\n",
    "\n",
    "    return Data(x=node_values, edge_index=adj_list_pt.t().contiguous(), edge_attr=edge_values, y=label)\n",
    "\n",
    "def loadData(root_dir):\n",
    "    graphs = list()\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        if len(dirs) > 0:\n",
    "            continue\n",
    "        print(subdir)\n",
    "        ids = list()\n",
    "        for file in tqdm(files):\n",
    "            file_id = int(file.split(\".\")[0])\n",
    "            if file_id in ids:\n",
    "                continue\n",
    "            ids.append(file_id)\n",
    "            graph = graphToData(subdir, file_id)\n",
    "            graphs.append(graph)\n",
    "        print(f\"Number of graphs: {len(ids)}\")\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNLayer(torch_geometric.nn.MessagePassing):\n",
    "    def __init__(self, num_node_features_in, num_node_features_out, num_edge_features, \n",
    "                 num_hidden_layers_message, num_hidden_layers_update,\n",
    "                 size_nn_message_hidden, size_nn_update_hidden):\n",
    "        super().__init__(aggr=\"add\", flow=\"source_to_target\") #source_to_target: create message to node i if (j,i) is edge\n",
    "        self.num_node_features_in = num_node_features_in\n",
    "        self.num_node_features_out = num_node_features_out\n",
    "        self.num_edge_features = num_edge_features\n",
    "\n",
    "        #message neural network:\n",
    "        #size of input layers is always 2*number of node features (in) + number of edge features\n",
    "        #size of output layer is always number of node features out\n",
    "        #size of hidden layers is always size_nn_message_hidden\n",
    "        self.layers_message = list()\n",
    "        self.layers_message.append(\n",
    "            torch.nn.Linear(in_features=2*self.num_node_features_in + self.num_edge_features, out_features=size_nn_message_hidden, bias=True)\n",
    "        )\n",
    "        self.layers_message.append(\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        for _ in range(num_hidden_layers_message - 1):\n",
    "            self.layers_message.append(\n",
    "                torch.nn.Linear(in_features=size_nn_message_hidden, out_features=size_nn_message_hidden, bias=True)\n",
    "            )\n",
    "            self.layers_message.append(\n",
    "                torch.nn.ReLU()\n",
    "            )\n",
    "        self.layers_message.append(\n",
    "            torch.nn.Linear(size_nn_message_hidden, out_features=num_node_features_out, bias=True)\n",
    "        )\n",
    "        self.nn_message = torch.nn.ModuleList(self.layers_message)\n",
    "\n",
    "        #update neural network:\n",
    "        #size of input layer is always number of node features out + number of node features in\n",
    "        #size of output layer is always number of node features out\n",
    "        #size of hidden layers is always size_nn_update_hidden\n",
    "\n",
    "        self.layers_update = list()\n",
    "        self.layers_update.append(\n",
    "            torch.nn.Linear(in_features=self.num_node_features_out + self.num_node_features_in, out_features=size_nn_update_hidden, bias=True),\n",
    "        )\n",
    "        self.layers_update.append(\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        for _ in range(num_hidden_layers_update - 1):\n",
    "            self.layers_update.append(\n",
    "                torch.nn.Linear(in_features=size_nn_update_hidden, out_features=size_nn_update_hidden, bias=True),\n",
    "            )\n",
    "            self.layers_update.append(\n",
    "                torch.nn.ReLU()\n",
    "            )\n",
    "        self.layers_update.append(\n",
    "            torch.nn.Linear(in_features=size_nn_update_hidden, out_features=num_node_features_out, bias=True)\n",
    "        )\n",
    "        self.nn_update = torch.nn.ModuleList(self.layers_update)\n",
    "    \n",
    "    def forward(self, x, edge_list, edge_attr):\n",
    "        out = self.propagate(edge_list, x=x, edge_attr=edge_attr) #calls message(), aggregate(), update()\n",
    "        return out #shape = [number of nodes, number of node features]\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        # _i = central node, _j = neighboring node\n",
    "        # x_i,j =[number of edges, number of node features]\n",
    "        # edge_attr = [number of edges, number of edge features]\n",
    "        # the node with node features x_i[k, :] is connected with the nodes having the features x_j[k, :]. The edge connecting these nodes has the features edge_attr[k,:]\n",
    "\n",
    "        vec_in = torch.cat((x_i, x_j, edge_attr), dim = 1) # shape = [num_edges, 2*number of node_features + number of edge_features]\n",
    "        #message = self.nn_message(vec_in) #shape = [num_edges, num node features]\n",
    "        for i in range(len(self.nn_message)):\n",
    "            vec_in = self.nn_message[i](vec_in)\n",
    "        return vec_in #return the message that is passed to node x_i\n",
    "\n",
    "    def update(self, input, x):\n",
    "        #input = output from aggregation step -> input shape = [number of nodes, number of node features]\n",
    "        #x_i shape = [number of nodes, number of node_features]\n",
    "        \n",
    "        vec_in = torch.cat((x, input), dim = 1) #shape = [number of nodes, 2* number of node features]\n",
    "        #updated_input = self.nn_update(vec_in) #shape = [number of nodes, number of node features]\n",
    "        for i in range(len(self.nn_update)):\n",
    "            vec_in = self.nn_update[i](vec_in)\n",
    "        return vec_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes, num_edge_features, \n",
    "                 num_additional_layers, num_hidden_layers_message, num_hidden_layers_update,\n",
    "                 width_nn_message_hidden, width_nn_update_hidden):\n",
    "        super().__init__()\n",
    "        #from GNN Layer: \n",
    "        # def __init__(self, num_node_features_in, num_node_features_out, num_edge_features, \n",
    "        #         num_hidden_layers_message, num_hidden_layers_update,\n",
    "        #         size_nn_message_hidden, size_nn_update_hidden):\n",
    "\n",
    "        self.first_layer = GNNLayer(num_node_features, num_classes, num_edge_features,\n",
    "                                    num_hidden_layers_message, num_hidden_layers_update,\n",
    "                                    width_nn_message_hidden, width_nn_update_hidden)\n",
    "        \n",
    "        #self.pool1 = torch_geometric.nn.pool.TopKPooling(in_channels=num_classes, ratio=0.5)\n",
    "\n",
    "        self.layers = list()\n",
    "        for _ in range(num_additional_layers):\n",
    "            self.layers.append(GNNLayer(num_classes, num_classes, num_edge_features, \n",
    "                                        num_hidden_layers_message, num_hidden_layers_update,\n",
    "                                        width_nn_message_hidden, width_nn_update_hidden\n",
    "                                        ))\n",
    "        self.layers = torch.nn.ModuleList(self.layers)\n",
    "        \n",
    "        self.last_layer = GNNLayer(num_classes, num_classes, num_edge_features, \n",
    "                                   num_hidden_layers_message, num_hidden_layers_update,\n",
    "                                   width_nn_message_hidden, width_nn_update_hidden)\n",
    "        \n",
    "    def forward(self, batch_dat):\n",
    "        x, edge_list, edge_attr, batch = batch_dat.x, batch_dat.edge_index, batch_dat.edge_attr, batch_dat.batch\n",
    "        x = self.first_layer(x, edge_list, edge_attr)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        \n",
    "        #x, edge_list, edge_attr, batch, perm, score = self.pool1(x, edge_list, edge_attr=edge_attr, batch=batch)\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x, edge_list, edge_attr)\n",
    "            x = torch.nn.functional.relu(x)\n",
    "\n",
    "        x = self.last_layer(x, edge_list, edge_attr) #shape=[number of nodes, number of node features=number of classe]\n",
    "        logits = torch_geometric.nn.global_mean_pool(x, batch) #shape [number of batches, number of classes]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, model, loss_fn, optimizer, device, save=False, file_save=\"\"):\n",
    "    total_num_dataset = len(loader.dataset)\n",
    "    model.train()\n",
    "    loss_save = list()\n",
    "    for batch_nr, batch_dat in enumerate(loader):\n",
    "        batch_dat = batch_dat.to(device)\n",
    "        pred = model(batch_dat)\n",
    "        loss = loss_fn(pred, batch_dat.y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch_nr % 50 == 0:\n",
    "            loss_, current = loss.item(), (batch_nr + 1)*len(batch_dat)\n",
    "            print(f\"loss: {loss_:>7f} [{current:>5d}/{total_num_dataset:>5d}]\")\n",
    "        if save:\n",
    "            loss_ = loss.item()\n",
    "            loss_save.append(loss_)\n",
    "    if save:\n",
    "        np.savetxt(fname=file_save, X=loss_save)\n",
    "\n",
    "\n",
    "def test(loader, model, loss_fn, device, save=False, file_save=\"\"):\n",
    "    size = len(loader.dataset)\n",
    "    num_batches = len(loader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            test_loss += loss_fn(pred, batch.y).item()\n",
    "            correct += (pred.argmax(dim=1) == batch.y.argmax(dim=1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /=size\n",
    "    print(f\"Test Error:\\n Accuracy: {(100*correct):>0.1f}%, Avg_loss: {test_loss:>8f}\\n\")\n",
    "    if save:\n",
    "        f = open(file_save, \"a+\")\n",
    "        f.write(f\"{test_loss},{correct}\\n\")\n",
    "        f.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/DataSet1/no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 287.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/DataSet1/yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 274.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 8\n",
      "./Data/DataSet2/no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1644/1644 [00:04<00:00, 372.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 822\n",
      "./Data/DataSet2/yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1586/1586 [00:04<00:00, 331.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "import random\n",
    "data_list = loadData(\"./Data\")\n",
    "random.shuffle(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1304\n",
      "DataBatch(x=[800, 2], edge_index=[2, 1528], edge_attr=[1528, 1], y=[8, 2], batch=[800], ptr=[9])\n"
     ]
    }
   ],
   "source": [
    "#split data intpo test and training set (80%:20%)\n",
    "train_dataloader = DataLoader(data_list[:int(0.8*len(data_list))], batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(data_list[int(0.8*len(data_list)):], batch_size=8, shuffle=True)\n",
    "print(len(train_dataloader.dataset))\n",
    "print(next(iter(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Without Training\n",
      "----------------------------------------\n",
      "Test Error:\n",
      " Accuracy: 49.1%, Avg_loss: 0.702485\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "----------------------------------------\n",
      "loss: 0.670685 [    8/ 1304]\n",
      "loss: 0.679671 [  408/ 1304]\n",
      "loss: 0.670344 [  808/ 1304]\n",
      "loss: 0.600240 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 62.9%, Avg_loss: 0.666906\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "----------------------------------------\n",
      "loss: 0.724499 [    8/ 1304]\n",
      "loss: 0.659035 [  408/ 1304]\n",
      "loss: 0.641504 [  808/ 1304]\n",
      "loss: 0.702148 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 56.7%, Avg_loss: 0.661469\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "----------------------------------------\n",
      "loss: 0.719518 [    8/ 1304]\n",
      "loss: 0.615152 [  408/ 1304]\n",
      "loss: 0.540360 [  808/ 1304]\n",
      "loss: 0.569129 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 50.9%, Avg_loss: 0.732179\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "----------------------------------------\n",
      "loss: 0.712848 [    8/ 1304]\n",
      "loss: 0.655357 [  408/ 1304]\n",
      "loss: 0.545332 [  808/ 1304]\n",
      "loss: 0.687733 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 62.3%, Avg_loss: 0.640951\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "----------------------------------------\n",
      "loss: 0.700878 [    8/ 1304]\n",
      "loss: 0.702896 [  408/ 1304]\n",
      "loss: 0.603680 [  808/ 1304]\n",
      "loss: 0.787675 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 65.0%, Avg_loss: 0.623529\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "----------------------------------------\n",
      "loss: 0.493132 [    8/ 1304]\n",
      "loss: 0.694031 [  408/ 1304]\n",
      "loss: 0.617372 [  808/ 1304]\n",
      "loss: 0.645299 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 65.3%, Avg_loss: 0.626280\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "----------------------------------------\n",
      "loss: 0.500705 [    8/ 1304]\n",
      "loss: 0.561125 [  408/ 1304]\n",
      "loss: 0.687919 [  808/ 1304]\n",
      "loss: 0.739869 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 65.6%, Avg_loss: 0.620766\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "----------------------------------------\n",
      "loss: 0.525126 [    8/ 1304]\n",
      "loss: 0.828907 [  408/ 1304]\n",
      "loss: 0.474926 [  808/ 1304]\n",
      "loss: 0.769762 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 65.6%, Avg_loss: 0.605359\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "----------------------------------------\n",
      "loss: 0.804200 [    8/ 1304]\n",
      "loss: 0.532507 [  408/ 1304]\n",
      "loss: 0.425609 [  808/ 1304]\n",
      "loss: 0.658151 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 68.1%, Avg_loss: 0.583199\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "----------------------------------------\n",
      "loss: 0.500543 [    8/ 1304]\n",
      "loss: 0.480969 [  408/ 1304]\n",
      "loss: 0.573709 [  808/ 1304]\n",
      "loss: 0.800554 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 67.5%, Avg_loss: 0.582749\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "----------------------------------------\n",
      "loss: 0.492557 [    8/ 1304]\n",
      "loss: 0.380399 [  408/ 1304]\n",
      "loss: 0.689664 [  808/ 1304]\n",
      "loss: 0.344757 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 67.2%, Avg_loss: 0.582473\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "----------------------------------------\n",
      "loss: 0.568243 [    8/ 1304]\n",
      "loss: 0.755766 [  408/ 1304]\n",
      "loss: 0.555568 [  808/ 1304]\n",
      "loss: 0.808967 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 67.5%, Avg_loss: 0.578228\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "----------------------------------------\n",
      "loss: 0.648864 [    8/ 1304]\n",
      "loss: 0.512188 [  408/ 1304]\n",
      "loss: 0.700608 [  808/ 1304]\n",
      "loss: 0.510312 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 67.5%, Avg_loss: 0.559784\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "----------------------------------------\n",
      "loss: 0.534474 [    8/ 1304]\n",
      "loss: 0.350955 [  408/ 1304]\n",
      "loss: 0.687543 [  808/ 1304]\n",
      "loss: 0.780084 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 70.6%, Avg_loss: 0.556767\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "----------------------------------------\n",
      "loss: 0.419962 [    8/ 1304]\n",
      "loss: 0.586866 [  408/ 1304]\n",
      "loss: 0.332992 [  808/ 1304]\n",
      "loss: 0.487314 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.3%, Avg_loss: 0.559112\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "----------------------------------------\n",
      "loss: 0.865944 [    8/ 1304]\n",
      "loss: 0.813958 [  408/ 1304]\n",
      "loss: 0.493425 [  808/ 1304]\n",
      "loss: 0.513990 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 68.7%, Avg_loss: 0.585153\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "----------------------------------------\n",
      "loss: 0.935051 [    8/ 1304]\n",
      "loss: 0.467605 [  408/ 1304]\n",
      "loss: 0.731471 [  808/ 1304]\n",
      "loss: 0.381914 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.6%, Avg_loss: 0.550614\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "----------------------------------------\n",
      "loss: 0.578804 [    8/ 1304]\n",
      "loss: 0.529105 [  408/ 1304]\n",
      "loss: 0.428470 [  808/ 1304]\n",
      "loss: 0.598516 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.6%, Avg_loss: 0.552988\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "----------------------------------------\n",
      "loss: 0.726106 [    8/ 1304]\n",
      "loss: 0.286455 [  408/ 1304]\n",
      "loss: 0.519595 [  808/ 1304]\n",
      "loss: 0.516197 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 68.1%, Avg_loss: 0.593332\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "----------------------------------------\n",
      "loss: 1.121727 [    8/ 1304]\n",
      "loss: 0.517431 [  408/ 1304]\n",
      "loss: 0.507979 [  808/ 1304]\n",
      "loss: 0.651776 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.9%, Avg_loss: 0.548466\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "----------------------------------------\n",
      "loss: 0.456577 [    8/ 1304]\n",
      "loss: 0.459318 [  408/ 1304]\n",
      "loss: 0.719290 [  808/ 1304]\n",
      "loss: 0.561205 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 70.9%, Avg_loss: 0.564823\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "----------------------------------------\n",
      "loss: 0.597847 [    8/ 1304]\n",
      "loss: 0.534622 [  408/ 1304]\n",
      "loss: 0.322785 [  808/ 1304]\n",
      "loss: 0.410369 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 70.2%, Avg_loss: 0.549354\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "----------------------------------------\n",
      "loss: 0.390114 [    8/ 1304]\n",
      "loss: 0.511047 [  408/ 1304]\n",
      "loss: 0.719085 [  808/ 1304]\n",
      "loss: 0.949451 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.6%, Avg_loss: 0.563068\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "----------------------------------------\n",
      "loss: 0.410634 [    8/ 1304]\n",
      "loss: 0.883033 [  408/ 1304]\n",
      "loss: 0.469933 [  808/ 1304]\n",
      "loss: 0.526983 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.6%, Avg_loss: 0.572375\n",
      "\n",
      "\n",
      "Epoch 25\n",
      "----------------------------------------\n",
      "loss: 0.738896 [    8/ 1304]\n",
      "loss: 0.426051 [  408/ 1304]\n",
      "loss: 0.526980 [  808/ 1304]\n",
      "loss: 0.611068 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 72.1%, Avg_loss: 0.547983\n",
      "\n",
      "\n",
      "Epoch 26\n",
      "----------------------------------------\n",
      "loss: 0.562205 [    8/ 1304]\n",
      "loss: 0.894717 [  408/ 1304]\n",
      "loss: 0.498514 [  808/ 1304]\n",
      "loss: 0.594005 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 70.6%, Avg_loss: 0.563596\n",
      "\n",
      "\n",
      "Epoch 27\n",
      "----------------------------------------\n",
      "loss: 0.480235 [    8/ 1304]\n",
      "loss: 0.833657 [  408/ 1304]\n",
      "loss: 0.544076 [  808/ 1304]\n",
      "loss: 0.541836 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 67.5%, Avg_loss: 0.548332\n",
      "\n",
      "\n",
      "Epoch 28\n",
      "----------------------------------------\n",
      "loss: 0.409364 [    8/ 1304]\n",
      "loss: 0.336323 [  408/ 1304]\n",
      "loss: 0.221495 [  808/ 1304]\n",
      "loss: 0.413044 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 71.2%, Avg_loss: 0.556482\n",
      "\n",
      "\n",
      "Epoch 29\n",
      "----------------------------------------\n",
      "loss: 0.798876 [    8/ 1304]\n",
      "loss: 0.395459 [  408/ 1304]\n",
      "loss: 0.468408 [  808/ 1304]\n",
      "loss: 0.281653 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.3%, Avg_loss: 0.546638\n",
      "\n",
      "\n",
      "Epoch 30\n",
      "----------------------------------------\n",
      "loss: 0.681669 [    8/ 1304]\n",
      "loss: 0.366608 [  408/ 1304]\n",
      "loss: 0.770910 [  808/ 1304]\n",
      "loss: 0.494751 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 67.2%, Avg_loss: 0.577994\n",
      "\n",
      "\n",
      "Epoch 31\n",
      "----------------------------------------\n",
      "loss: 0.409192 [    8/ 1304]\n",
      "loss: 0.640204 [  408/ 1304]\n",
      "loss: 0.427543 [  808/ 1304]\n",
      "loss: 0.485374 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 72.1%, Avg_loss: 0.548463\n",
      "\n",
      "\n",
      "Epoch 32\n",
      "----------------------------------------\n",
      "loss: 0.314321 [    8/ 1304]\n",
      "loss: 0.480475 [  408/ 1304]\n",
      "loss: 0.410323 [  808/ 1304]\n",
      "loss: 0.568960 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.3%, Avg_loss: 0.543671\n",
      "\n",
      "\n",
      "Epoch 33\n",
      "----------------------------------------\n",
      "loss: 0.684017 [    8/ 1304]\n",
      "loss: 0.515562 [  408/ 1304]\n",
      "loss: 0.363326 [  808/ 1304]\n",
      "loss: 0.292402 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 70.2%, Avg_loss: 0.544029\n",
      "\n",
      "\n",
      "Epoch 34\n",
      "----------------------------------------\n",
      "loss: 0.566315 [    8/ 1304]\n",
      "loss: 0.593894 [  408/ 1304]\n",
      "loss: 0.697651 [  808/ 1304]\n",
      "loss: 0.793354 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 71.5%, Avg_loss: 0.543708\n",
      "\n",
      "\n",
      "Epoch 35\n",
      "----------------------------------------\n",
      "loss: 0.403555 [    8/ 1304]\n",
      "loss: 0.604612 [  408/ 1304]\n",
      "loss: 0.780428 [  808/ 1304]\n",
      "loss: 0.540941 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.9%, Avg_loss: 0.539946\n",
      "\n",
      "\n",
      "Epoch 36\n",
      "----------------------------------------\n",
      "loss: 0.400234 [    8/ 1304]\n",
      "loss: 0.746227 [  408/ 1304]\n",
      "loss: 0.461515 [  808/ 1304]\n",
      "loss: 0.507877 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.6%, Avg_loss: 0.565881\n",
      "\n",
      "\n",
      "Epoch 37\n",
      "----------------------------------------\n",
      "loss: 0.489576 [    8/ 1304]\n",
      "loss: 0.695001 [  408/ 1304]\n",
      "loss: 0.492279 [  808/ 1304]\n",
      "loss: 0.460221 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.6%, Avg_loss: 0.548233\n",
      "\n",
      "\n",
      "Epoch 38\n",
      "----------------------------------------\n",
      "loss: 0.666394 [    8/ 1304]\n",
      "loss: 0.474646 [  408/ 1304]\n",
      "loss: 0.567060 [  808/ 1304]\n",
      "loss: 0.670313 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 71.5%, Avg_loss: 0.542545\n",
      "\n",
      "\n",
      "Epoch 39\n",
      "----------------------------------------\n",
      "loss: 0.604310 [    8/ 1304]\n",
      "loss: 0.544417 [  408/ 1304]\n",
      "loss: 0.367657 [  808/ 1304]\n",
      "loss: 0.614415 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 70.9%, Avg_loss: 0.542700\n",
      "\n",
      "\n",
      "Epoch 40\n",
      "----------------------------------------\n",
      "loss: 0.183647 [    8/ 1304]\n",
      "loss: 0.563280 [  408/ 1304]\n",
      "loss: 0.619398 [  808/ 1304]\n",
      "loss: 0.529481 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 70.2%, Avg_loss: 0.545874\n",
      "\n",
      "\n",
      "Epoch 41\n",
      "----------------------------------------\n",
      "loss: 0.656440 [    8/ 1304]\n",
      "loss: 0.364788 [  408/ 1304]\n",
      "loss: 0.452928 [  808/ 1304]\n",
      "loss: 0.369784 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 66.6%, Avg_loss: 0.550623\n",
      "\n",
      "\n",
      "Epoch 42\n",
      "----------------------------------------\n",
      "loss: 0.710459 [    8/ 1304]\n",
      "loss: 0.466988 [  408/ 1304]\n",
      "loss: 0.363422 [  808/ 1304]\n",
      "loss: 0.740688 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 70.6%, Avg_loss: 0.543766\n",
      "\n",
      "\n",
      "Epoch 43\n",
      "----------------------------------------\n",
      "loss: 0.524956 [    8/ 1304]\n",
      "loss: 0.699553 [  408/ 1304]\n",
      "loss: 0.453558 [  808/ 1304]\n",
      "loss: 0.549529 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 71.2%, Avg_loss: 0.552533\n",
      "\n",
      "\n",
      "Epoch 44\n",
      "----------------------------------------\n",
      "loss: 0.683347 [    8/ 1304]\n",
      "loss: 0.586210 [  408/ 1304]\n",
      "loss: 0.402328 [  808/ 1304]\n",
      "loss: 0.553792 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.3%, Avg_loss: 0.542195\n",
      "\n",
      "\n",
      "Epoch 45\n",
      "----------------------------------------\n",
      "loss: 0.442980 [    8/ 1304]\n",
      "loss: 0.585210 [  408/ 1304]\n",
      "loss: 0.387049 [  808/ 1304]\n",
      "loss: 0.352134 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 70.6%, Avg_loss: 0.547663\n",
      "\n",
      "\n",
      "Epoch 46\n",
      "----------------------------------------\n",
      "loss: 0.700288 [    8/ 1304]\n",
      "loss: 0.490877 [  408/ 1304]\n",
      "loss: 0.617602 [  808/ 1304]\n",
      "loss: 0.564911 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 71.2%, Avg_loss: 0.546016\n",
      "\n",
      "\n",
      "Epoch 47\n",
      "----------------------------------------\n",
      "loss: 0.482633 [    8/ 1304]\n",
      "loss: 0.672738 [  408/ 1304]\n",
      "loss: 0.534126 [  808/ 1304]\n",
      "loss: 0.364812 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.0%, Avg_loss: 0.559206\n",
      "\n",
      "\n",
      "Epoch 48\n",
      "----------------------------------------\n",
      "loss: 0.407994 [    8/ 1304]\n",
      "loss: 0.857152 [  408/ 1304]\n",
      "loss: 0.489020 [  808/ 1304]\n",
      "loss: 0.496777 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.9%, Avg_loss: 0.564711\n",
      "\n",
      "\n",
      "Epoch 49\n",
      "----------------------------------------\n",
      "loss: 0.628876 [    8/ 1304]\n",
      "loss: 0.596120 [  408/ 1304]\n",
      "loss: 0.749873 [  808/ 1304]\n",
      "loss: 0.373924 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 70.2%, Avg_loss: 0.543134\n",
      "\n",
      "\n",
      "Epoch 50\n",
      "----------------------------------------\n",
      "loss: 0.427755 [    8/ 1304]\n",
      "loss: 0.633766 [  408/ 1304]\n",
      "loss: 0.630507 [  808/ 1304]\n",
      "loss: 0.674262 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.3%, Avg_loss: 0.540634\n",
      "\n",
      "\n",
      "Epoch 51\n",
      "----------------------------------------\n",
      "loss: 0.562542 [    8/ 1304]\n",
      "loss: 0.480960 [  408/ 1304]\n",
      "loss: 0.565153 [  808/ 1304]\n",
      "loss: 0.612786 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 67.8%, Avg_loss: 0.547775\n",
      "\n",
      "\n",
      "Epoch 52\n",
      "----------------------------------------\n",
      "loss: 0.515757 [    8/ 1304]\n",
      "loss: 0.462535 [  408/ 1304]\n",
      "loss: 0.658246 [  808/ 1304]\n",
      "loss: 0.395787 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.9%, Avg_loss: 0.544103\n",
      "\n",
      "\n",
      "Epoch 53\n",
      "----------------------------------------\n",
      "loss: 0.424758 [    8/ 1304]\n",
      "loss: 0.584922 [  408/ 1304]\n",
      "loss: 0.501910 [  808/ 1304]\n",
      "loss: 0.767726 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 70.9%, Avg_loss: 0.553467\n",
      "\n",
      "\n",
      "Epoch 54\n",
      "----------------------------------------\n",
      "loss: 0.519851 [    8/ 1304]\n",
      "loss: 0.256170 [  408/ 1304]\n",
      "loss: 0.556352 [  808/ 1304]\n",
      "loss: 0.496892 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.9%, Avg_loss: 0.541214\n",
      "\n",
      "\n",
      "Epoch 55\n",
      "----------------------------------------\n",
      "loss: 0.315560 [    8/ 1304]\n",
      "loss: 0.894331 [  408/ 1304]\n",
      "loss: 0.273455 [  808/ 1304]\n",
      "loss: 0.646469 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.3%, Avg_loss: 0.537373\n",
      "\n",
      "\n",
      "Epoch 56\n",
      "----------------------------------------\n",
      "loss: 0.613986 [    8/ 1304]\n",
      "loss: 0.387432 [  408/ 1304]\n",
      "loss: 0.480940 [  808/ 1304]\n",
      "loss: 0.464608 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 71.2%, Avg_loss: 0.544843\n",
      "\n",
      "\n",
      "Epoch 57\n",
      "----------------------------------------\n",
      "loss: 0.584906 [    8/ 1304]\n",
      "loss: 0.450782 [  408/ 1304]\n",
      "loss: 0.855490 [  808/ 1304]\n",
      "loss: 0.655471 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 71.2%, Avg_loss: 0.548562\n",
      "\n",
      "\n",
      "Epoch 58\n",
      "----------------------------------------\n",
      "loss: 0.496968 [    8/ 1304]\n",
      "loss: 0.668816 [  408/ 1304]\n",
      "loss: 0.493304 [  808/ 1304]\n",
      "loss: 0.358912 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.0%, Avg_loss: 0.532880\n",
      "\n",
      "\n",
      "Epoch 59\n",
      "----------------------------------------\n",
      "loss: 0.320214 [    8/ 1304]\n",
      "loss: 0.432801 [  408/ 1304]\n",
      "loss: 0.411384 [  808/ 1304]\n",
      "loss: 0.264816 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 71.5%, Avg_loss: 0.547394\n",
      "\n",
      "\n",
      "Epoch 60\n",
      "----------------------------------------\n",
      "loss: 0.525216 [    8/ 1304]\n",
      "loss: 0.444460 [  408/ 1304]\n",
      "loss: 0.543832 [  808/ 1304]\n",
      "loss: 0.877615 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.3%, Avg_loss: 0.538128\n",
      "\n",
      "\n",
      "Epoch 61\n",
      "----------------------------------------\n",
      "loss: 0.398547 [    8/ 1304]\n",
      "loss: 0.492295 [  408/ 1304]\n",
      "loss: 0.461752 [  808/ 1304]\n",
      "loss: 0.609810 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.3%, Avg_loss: 0.575481\n",
      "\n",
      "\n",
      "Epoch 62\n",
      "----------------------------------------\n",
      "loss: 0.665235 [    8/ 1304]\n",
      "loss: 0.733923 [  408/ 1304]\n",
      "loss: 0.319288 [  808/ 1304]\n",
      "loss: 0.713651 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 71.2%, Avg_loss: 0.543711\n",
      "\n",
      "\n",
      "Epoch 63\n",
      "----------------------------------------\n",
      "loss: 0.460133 [    8/ 1304]\n",
      "loss: 0.486706 [  408/ 1304]\n",
      "loss: 0.787210 [  808/ 1304]\n",
      "loss: 0.967246 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 68.7%, Avg_loss: 0.545508\n",
      "\n",
      "\n",
      "Epoch 64\n",
      "----------------------------------------\n",
      "loss: 0.622607 [    8/ 1304]\n",
      "loss: 0.591315 [  408/ 1304]\n",
      "loss: 0.544584 [  808/ 1304]\n",
      "loss: 0.269242 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 71.5%, Avg_loss: 0.540579\n",
      "\n",
      "\n",
      "Epoch 65\n",
      "----------------------------------------\n",
      "loss: 0.314030 [    8/ 1304]\n",
      "loss: 0.485605 [  408/ 1304]\n",
      "loss: 0.438329 [  808/ 1304]\n",
      "loss: 0.270135 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 71.2%, Avg_loss: 0.525139\n",
      "\n",
      "\n",
      "Epoch 66\n",
      "----------------------------------------\n",
      "loss: 0.450780 [    8/ 1304]\n",
      "loss: 0.506389 [  408/ 1304]\n",
      "loss: 0.570924 [  808/ 1304]\n",
      "loss: 0.431372 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 69.6%, Avg_loss: 0.548837\n",
      "\n",
      "\n",
      "Epoch 67\n",
      "----------------------------------------\n",
      "loss: 0.887902 [    8/ 1304]\n",
      "loss: 0.552463 [  408/ 1304]\n",
      "loss: 0.386770 [  808/ 1304]\n",
      "loss: 0.394652 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 71.5%, Avg_loss: 0.544344\n",
      "\n",
      "\n",
      "Epoch 68\n",
      "----------------------------------------\n",
      "loss: 0.600994 [    8/ 1304]\n",
      "loss: 0.376602 [  408/ 1304]\n",
      "loss: 0.949254 [  808/ 1304]\n",
      "loss: 0.662328 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 72.1%, Avg_loss: 0.523195\n",
      "\n",
      "\n",
      "Epoch 69\n",
      "----------------------------------------\n",
      "loss: 0.500265 [    8/ 1304]\n",
      "loss: 0.518537 [  408/ 1304]\n",
      "loss: 0.453545 [  808/ 1304]\n",
      "loss: 0.901583 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 68.7%, Avg_loss: 0.531189\n",
      "\n",
      "\n",
      "Epoch 70\n",
      "----------------------------------------\n",
      "loss: 0.512907 [    8/ 1304]\n",
      "loss: 0.489841 [  408/ 1304]\n",
      "loss: 0.353848 [  808/ 1304]\n",
      "loss: 0.597582 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 73.0%, Avg_loss: 0.516787\n",
      "\n",
      "\n",
      "Epoch 71\n",
      "----------------------------------------\n",
      "loss: 0.565790 [    8/ 1304]\n",
      "loss: 0.556464 [  408/ 1304]\n",
      "loss: 0.573639 [  808/ 1304]\n",
      "loss: 0.729255 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 73.3%, Avg_loss: 0.524979\n",
      "\n",
      "\n",
      "Epoch 72\n",
      "----------------------------------------\n",
      "loss: 0.380083 [    8/ 1304]\n",
      "loss: 0.460880 [  408/ 1304]\n",
      "loss: 0.430819 [  808/ 1304]\n",
      "loss: 0.548358 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 74.2%, Avg_loss: 0.496971\n",
      "\n",
      "\n",
      "Epoch 73\n",
      "----------------------------------------\n",
      "loss: 0.322057 [    8/ 1304]\n",
      "loss: 0.476837 [  408/ 1304]\n",
      "loss: 0.643942 [  808/ 1304]\n",
      "loss: 0.443466 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 74.8%, Avg_loss: 0.489518\n",
      "\n",
      "\n",
      "Epoch 74\n",
      "----------------------------------------\n",
      "loss: 0.488610 [    8/ 1304]\n",
      "loss: 0.401339 [  408/ 1304]\n",
      "loss: 0.360626 [  808/ 1304]\n",
      "loss: 0.695265 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 71.2%, Avg_loss: 0.526893\n",
      "\n",
      "\n",
      "Epoch 75\n",
      "----------------------------------------\n",
      "loss: 0.552667 [    8/ 1304]\n",
      "loss: 0.279936 [  408/ 1304]\n",
      "loss: 0.487667 [  808/ 1304]\n",
      "loss: 0.345719 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 71.2%, Avg_loss: 0.505707\n",
      "\n",
      "\n",
      "Epoch 76\n",
      "----------------------------------------\n",
      "loss: 0.532501 [    8/ 1304]\n",
      "loss: 0.573693 [  408/ 1304]\n",
      "loss: 0.503816 [  808/ 1304]\n",
      "loss: 0.455583 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 72.1%, Avg_loss: 0.519513\n",
      "\n",
      "\n",
      "Epoch 77\n",
      "----------------------------------------\n",
      "loss: 0.591043 [    8/ 1304]\n",
      "loss: 0.561586 [  408/ 1304]\n",
      "loss: 0.592235 [  808/ 1304]\n",
      "loss: 0.335204 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 76.4%, Avg_loss: 0.488877\n",
      "\n",
      "\n",
      "Epoch 78\n",
      "----------------------------------------\n",
      "loss: 0.406596 [    8/ 1304]\n",
      "loss: 0.226802 [  408/ 1304]\n",
      "loss: 0.822376 [  808/ 1304]\n",
      "loss: 0.600198 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 75.5%, Avg_loss: 0.474828\n",
      "\n",
      "\n",
      "Epoch 79\n",
      "----------------------------------------\n",
      "loss: 0.395573 [    8/ 1304]\n",
      "loss: 0.437526 [  408/ 1304]\n",
      "loss: 0.704200 [  808/ 1304]\n",
      "loss: 0.675393 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 74.8%, Avg_loss: 0.485130\n",
      "\n",
      "\n",
      "Epoch 80\n",
      "----------------------------------------\n",
      "loss: 0.251517 [    8/ 1304]\n",
      "loss: 0.343825 [  408/ 1304]\n",
      "loss: 0.629527 [  808/ 1304]\n",
      "loss: 0.174347 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 76.1%, Avg_loss: 0.454190\n",
      "\n",
      "\n",
      "Epoch 81\n",
      "----------------------------------------\n",
      "loss: 0.601863 [    8/ 1304]\n",
      "loss: 0.452228 [  408/ 1304]\n",
      "loss: 0.686508 [  808/ 1304]\n",
      "loss: 0.447516 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 78.5%, Avg_loss: 0.467947\n",
      "\n",
      "\n",
      "Epoch 82\n",
      "----------------------------------------\n",
      "loss: 0.533255 [    8/ 1304]\n",
      "loss: 0.610343 [  408/ 1304]\n",
      "loss: 0.397517 [  808/ 1304]\n",
      "loss: 1.008590 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 75.5%, Avg_loss: 0.463059\n",
      "\n",
      "\n",
      "Epoch 83\n",
      "----------------------------------------\n",
      "loss: 0.509353 [    8/ 1304]\n",
      "loss: 0.366260 [  408/ 1304]\n",
      "loss: 0.840397 [  808/ 1304]\n",
      "loss: 0.206658 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 76.1%, Avg_loss: 0.463439\n",
      "\n",
      "\n",
      "Epoch 84\n",
      "----------------------------------------\n",
      "loss: 0.223581 [    8/ 1304]\n",
      "loss: 0.525712 [  408/ 1304]\n",
      "loss: 0.566653 [  808/ 1304]\n",
      "loss: 0.344603 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 76.7%, Avg_loss: 0.462408\n",
      "\n",
      "\n",
      "Epoch 85\n",
      "----------------------------------------\n",
      "loss: 0.791187 [    8/ 1304]\n",
      "loss: 0.562096 [  408/ 1304]\n",
      "loss: 0.481196 [  808/ 1304]\n",
      "loss: 0.594164 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 73.9%, Avg_loss: 0.465875\n",
      "\n",
      "\n",
      "Epoch 86\n",
      "----------------------------------------\n",
      "loss: 0.696533 [    8/ 1304]\n",
      "loss: 0.514821 [  408/ 1304]\n",
      "loss: 0.626804 [  808/ 1304]\n",
      "loss: 0.334066 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 75.5%, Avg_loss: 0.453690\n",
      "\n",
      "\n",
      "Epoch 87\n",
      "----------------------------------------\n",
      "loss: 0.299091 [    8/ 1304]\n",
      "loss: 0.424598 [  408/ 1304]\n",
      "loss: 0.930863 [  808/ 1304]\n",
      "loss: 0.382320 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 75.2%, Avg_loss: 0.484880\n",
      "\n",
      "\n",
      "Epoch 88\n",
      "----------------------------------------\n",
      "loss: 0.425547 [    8/ 1304]\n",
      "loss: 0.553159 [  408/ 1304]\n",
      "loss: 0.635642 [  808/ 1304]\n",
      "loss: 0.601768 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 76.4%, Avg_loss: 0.457342\n",
      "\n",
      "\n",
      "Epoch 89\n",
      "----------------------------------------\n",
      "loss: 0.682232 [    8/ 1304]\n",
      "loss: 0.683647 [  408/ 1304]\n",
      "loss: 0.473780 [  808/ 1304]\n",
      "loss: 0.248484 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 77.0%, Avg_loss: 0.457593\n",
      "\n",
      "\n",
      "Epoch 90\n",
      "----------------------------------------\n",
      "loss: 0.466482 [    8/ 1304]\n",
      "loss: 0.640405 [  408/ 1304]\n",
      "loss: 0.413455 [  808/ 1304]\n",
      "loss: 0.573632 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 77.9%, Avg_loss: 0.450996\n",
      "\n",
      "\n",
      "Epoch 91\n",
      "----------------------------------------\n",
      "loss: 0.587019 [    8/ 1304]\n",
      "loss: 0.671433 [  408/ 1304]\n",
      "loss: 0.445970 [  808/ 1304]\n",
      "loss: 0.310572 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 77.6%, Avg_loss: 0.445951\n",
      "\n",
      "\n",
      "Epoch 92\n",
      "----------------------------------------\n",
      "loss: 0.351793 [    8/ 1304]\n",
      "loss: 0.424172 [  408/ 1304]\n",
      "loss: 0.413261 [  808/ 1304]\n",
      "loss: 1.024328 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 74.8%, Avg_loss: 0.471998\n",
      "\n",
      "\n",
      "Epoch 93\n",
      "----------------------------------------\n",
      "loss: 0.362310 [    8/ 1304]\n",
      "loss: 0.560027 [  408/ 1304]\n",
      "loss: 0.385813 [  808/ 1304]\n",
      "loss: 1.078345 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 72.4%, Avg_loss: 0.471847\n",
      "\n",
      "\n",
      "Epoch 94\n",
      "----------------------------------------\n",
      "loss: 0.465635 [    8/ 1304]\n",
      "loss: 0.565764 [  408/ 1304]\n",
      "loss: 0.814562 [  808/ 1304]\n",
      "loss: 0.423299 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 77.6%, Avg_loss: 0.442617\n",
      "\n",
      "\n",
      "Epoch 95\n",
      "----------------------------------------\n",
      "loss: 0.305140 [    8/ 1304]\n",
      "loss: 0.338713 [  408/ 1304]\n",
      "loss: 0.214942 [  808/ 1304]\n",
      "loss: 0.778651 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 77.9%, Avg_loss: 0.439547\n",
      "\n",
      "\n",
      "Epoch 96\n",
      "----------------------------------------\n",
      "loss: 0.210635 [    8/ 1304]\n",
      "loss: 0.723285 [  408/ 1304]\n",
      "loss: 0.389209 [  808/ 1304]\n",
      "loss: 0.501120 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 80.1%, Avg_loss: 0.430438\n",
      "\n",
      "\n",
      "Epoch 97\n",
      "----------------------------------------\n",
      "loss: 0.711667 [    8/ 1304]\n",
      "loss: 0.446299 [  408/ 1304]\n",
      "loss: 0.487621 [  808/ 1304]\n",
      "loss: 0.264291 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 81.6%, Avg_loss: 0.433057\n",
      "\n",
      "\n",
      "Epoch 98\n",
      "----------------------------------------\n",
      "loss: 0.385471 [    8/ 1304]\n",
      "loss: 0.360903 [  408/ 1304]\n",
      "loss: 0.555163 [  808/ 1304]\n",
      "loss: 0.343152 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 77.9%, Avg_loss: 0.436201\n",
      "\n",
      "\n",
      "Epoch 99\n",
      "----------------------------------------\n",
      "loss: 0.492874 [    8/ 1304]\n",
      "loss: 0.476281 [  408/ 1304]\n",
      "loss: 0.643416 [  808/ 1304]\n",
      "loss: 0.351227 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 76.4%, Avg_loss: 0.440621\n",
      "\n",
      "\n",
      "Epoch 100\n",
      "----------------------------------------\n",
      "loss: 0.672591 [    8/ 1304]\n",
      "loss: 0.371426 [  408/ 1304]\n",
      "loss: 0.653354 [  808/ 1304]\n",
      "loss: 0.476935 [ 1208/ 1304]\n",
      "Test Error:\n",
      " Accuracy: 78.2%, Avg_loss: 0.435674\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "model = Test(num_node_features=2, num_edge_features=1, num_classes=2,\n",
    "             num_additional_layers=10, num_hidden_layers_message=2, num_hidden_layers_update=1,\n",
    "             width_nn_message_hidden=30, width_nn_update_hidden=10)\n",
    "model.to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters())\n",
    "\n",
    "print(f\"\\nWithout Training\\n----------------------------------------\")\n",
    "test(test_dataloader, model, loss_fn, device)\n",
    "\n",
    "epochs = 100\n",
    "file_train = \"train_al10_1topk_100ep.txt\"\n",
    "file_test = \"test_al10_1topk_100ep.txt\"\n",
    "for t in range(epochs):\n",
    "    print(f\"\\nEpoch {t+1}\\n----------------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer, device, save=False, file_save=file_train)\n",
    "    test(test_dataloader, model, loss_fn, device, save=False, file_save=file_test)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
